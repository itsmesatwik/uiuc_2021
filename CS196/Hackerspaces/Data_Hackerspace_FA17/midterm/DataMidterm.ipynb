{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Hackerspace 'Midterm'\n",
    "** 15 points total **\n",
    "\n",
    "These problems are designed to be a bit different from the previous 'challenge' problems. Instead of instructing you to do something very specific, these problems are more general and approach-driven (like something you'd see on an actual interview, or in a Data Science job).\n",
    "\n",
    "Thus, the process of completing these problems is just as important as the 'correctness' of the answers. These questions are also a bit more open ended. Make the assumptions that you think are valid, and be sure to **comment in** those assumptions in your code.\n",
    "\n",
    "* Due Date: **November 29th, 11:59pm. No late Submissions.**\n",
    "* Credit is given for both accuracy, and a thoughtful algorithmic approach.\n",
    "* Submit this assignment emailing your completed `*.ipynb` to Tyler using the email we gave you in class (Keep your output!)\n",
    "\n",
    "## Problem 1: Email Address Matching\n",
    "** 4 points **\n",
    "\n",
    "You are given a list of names and a list of email addresses.  How would you automatically assign the 'best' email from the list of emails to the corresponding name from the list of names?\n",
    "\n",
    "You can find a list of names in `./names.txt` and the list of emails in `./emails.txt`.\n",
    "\n",
    "Output a list of `(name, email)` tuples for name/email pairs that you think match 'best'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This probably won't work in non-*nix operating systems\n",
    "!head names.txt\n",
    "!echo\n",
    "!head emails.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def email_matcher(emails_file, names_file):\n",
    "    pass\n",
    "\n",
    "email_matcher('emails.txt', 'names.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: San Francisco City Salaries\n",
    "** 2 points **\n",
    "\n",
    "In `Salaries.csv`, you'll find a list of pay data for the public employees of San Francisco City for the years 2011 - 2014\n",
    "\n",
    "Write a function that will take this data and plot the median pay change by occupation between 2011 and 2014. You can limit your visualization to the top 25 most populous professions.\n",
    "\n",
    "(Data courtesy of https://www.kaggle.com/kaggle/sf-salaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head Salaries.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def median_sf_salaries_change(filename):\n",
    "    pass\n",
    "\n",
    "median_sf_salaries_change('Salaries.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Airline Tweets\n",
    "** 3 points **\n",
    "\n",
    "Given a dataset of Tweets regarding different airlines (`Tweets.csv`), write a function that returns an ordered list of the most serious complaints against each airline.\n",
    "\n",
    "**Note:** The sentiment analysis has already been performed, and you are given a sentiment score, complaint label (`negativearesas`), and complaint label confidence markers for each tweet. For maximum accuracy, you may wish to utilize all three of these values in your analysis.\n",
    "\n",
    "(Data courtesy of https://www.kaggle.com/crowdflower/twitter-airline-sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head Tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def airline_complaints(filename, airline):\n",
    "    pass\n",
    "\n",
    "airline_complaints('Tweets.csv', 'American')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: City Name Cleaning\n",
    "** 3 points**\n",
    "\n",
    "Suppose you have a very large list of the names of all large cities in the world. (Number of records ~= 100,000) Many of these cities will contain common overlapping words like “Mobile”, “Rugby” or “Salmon”. Your task is to automatically (and hopefully quickly) filter out as many of these 'common' words as possible. Output a list of world cities that have these common words removed.\n",
    "\n",
    "Unfortunately, we don't have a clean list of cities, but rather a `*.csv` file that contains a bunch of other information. You can find this file in `worldcities.csv`\n",
    "\n",
    "(Data courtesy of https://www.maxmind.com/en/free-world-cities-database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail worldcities.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def city_name_cleanser(filename):\n",
    "    pass\n",
    "\n",
    "city_name_cleanser('worldcities.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: IMDB Crawler\n",
    "** 3 points **\n",
    "\n",
    "In an earlier challange problem, you wrote code to take an IMDB page and parse out specific information about the movie and cast.\n",
    "\n",
    "In this problem, you will go a step further and write a web-scraper to build a database of movie information.\n",
    "\n",
    "Write a function that takes as an argument a 'seed' URL (guaranteed to be somewhere on www.imdb.com) and crawls the movie links that it finds on that page. You should save the following characteristics about each movie:\n",
    "\n",
    "* Title\n",
    "* Rating\n",
    "* Duration\n",
    "* Release Date\n",
    "* Budget\n",
    "\n",
    "For the sake of time, you can also **limit your crawler to scraping and saving 25 movies.**\n",
    "\n",
    "The function should save the data to a JSON file.\n",
    "\n",
    "** Note: ** You're encouraged to separate your code into function(s) that scrape data from IMDB, and function(s) that perform the crawling logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imdb_crawler(seed_url, out_file):\n",
    "    pass\n",
    "\n",
    "imdb_crawler('http://www.imdb.com/', 'movies.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
